{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "m = MeCab.Tagger('-d /usr/lib/mecab/dic/mecab-ipadic-neologd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_df = pd.read_csv('example.csv',encoding=\"utf8\") \n",
    "len(csv_df)\n",
    "csv_df.info()\n",
    "csv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaconv\n",
    "wakatis = []\n",
    "words = []\n",
    "for w in csv_df['歳出事業']:\n",
    "    words1 = []\n",
    "    w = jaconv.z2h(w, digit=True, ascii=True) # 半角、小文字に統一\n",
    "    for c in m.parse(w).splitlines()[:-1]:  #事業名を分割し解析結果を配列にして繰り返す\n",
    "        surface, feature = c.split('\\t')    #分割された事業名と結果をそれぞれ格納 \n",
    "        if '名詞' in feature:             #名詞ならそれをwords1に追加            \n",
    "            if surface != '推進事業' and surface != '事業':\n",
    "                if surface == '健康診査':\n",
    "                    words1.append('健康')\n",
    "                    words1.append('診査')\n",
    "                elif surface == '庄内地域':\n",
    "                    words1.append('庄内')\n",
    "                    words1.append('地域')\n",
    "                else:\n",
    "                    words1.append(surface)\n",
    "            \n",
    "                \n",
    "    str1 = ' '.join(words1) #分割された事業名を空白で区切る\n",
    "    wakatis.append(str1)    #区切られた事業名を配列に格納\n",
    "    words.append(words1)\n",
    "# wakatis\n",
    "csv_df['wakati'] = wakatis\n",
    "learn_df = csv_df[['歳出事業コード','歳出事業','wakati']]\n",
    "learn_df.info()\n",
    "learn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer = 'word', max_df = 0.9)\n",
    "vecs = vectorizer.fit_transform(wakatis)\n",
    "\n",
    "model = word2vec.Word2Vec(words, size = 100, min_count = 1, window = 2, iter = 100)\n",
    "\n",
    "word_vectors = []\n",
    "for i in range(len(vectorizer.get_feature_names())):\n",
    "    if vectorizer.get_feature_names()[i] == 'uij':\n",
    "        word_vectors.append(model.wv['UIJ'])\n",
    "    else:\n",
    "        word_vectors.append(model.wv[vectorizer.get_feature_names()[i]])\n",
    "    \n",
    "word_vectors = np.array(word_vectors)\n",
    "vecs = np.array(vecs.toarray())\n",
    "word_vectors2 = np.dot(vecs, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    \n",
    "    word = input('検索:')\n",
    "    if word == '終了':\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        t1 = time.time()\n",
    "        words2 = []\n",
    "        wakatis2 = []\n",
    "        for c in m.parse(word).splitlines()[:-1]:\n",
    "            surface, feature = c.split('\\t')    #分割された事業名と結果をそれぞれ格納 \n",
    "            if '名詞' in feature:\n",
    "                wakatis2.append(surface)\n",
    "                words2.append([surface])\n",
    "                \n",
    "        str1 = ' '.join(wakatis2) #分割された事業名を空白で区切る\n",
    "        wakatis.append(str1)\n",
    "        \n",
    "        model.build_vocab(words2, update=True)\n",
    "        model.train(wakatis2, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "        vecs = vectorizer.fit_transform(wakatis)\n",
    "        \n",
    "        word_vectors = []\n",
    "        for i in range(len(vectorizer.get_feature_names())):\n",
    "            if vectorizer.get_feature_names()[i] == 'uij':\n",
    "                word_vectors.append(model.wv['UIJ'])\n",
    "            else:\n",
    "                word_vectors.append(model.wv[vectorizer.get_feature_names()[i]])\n",
    "    \n",
    "        word_vectors = np.array(word_vectors)\n",
    "        vecs = np.array(vecs.toarray())\n",
    "        word_vectors2 = np.dot(vecs, word_vectors)\n",
    "\n",
    "#        sim_list = []\n",
    "#        for jigyo in jigyo_list:\n",
    "        similarity = cosine_similarity(word_vectors2)[-1]\n",
    "#            sim_list.append(similarity)\n",
    "        topn_indices = np.argsort(similarity)[::-1][1:11]\n",
    "        \n",
    "        b = True\n",
    "        for sim, tweet in zip(similarity[topn_indices], np.array(csv_df['歳出事業'])[topn_indices]):\n",
    "            if sim != 0:\n",
    "                print(\"({:}): {}\".format(sim, \"\".join(tweet.split())))\n",
    "                b = False\n",
    "            if b:\n",
    "                print('検索結果なし')\n",
    "                break\n",
    "        del wakatis[-1]\n",
    "        print('検索時間:', time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
